#+Title: 数据格式
#+author: 续本达
#+PROPERTY: header-args :eval never-export :exports both

* 课前准备
  安装 HDF5 和 CSV 的相关工具。
  #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
    apt install vitables python3-h5py hdf5-tools csvtool
  #+end_src
* 数据格式
  到目前为止的输入和输出还比较简单和原始，形式上都是一个一个数字操作。如果这种数据达到几百TB，操作就太困难了。这就需要更高级的数据格式，作为中间结果的数据，最重要的是遵循透明原则。它默认由计算机处理，透明原则要求它易于被理解和检查。我们重点学习满足透明原则的三个范例 CSV、HDF5 和 JSON。
  程序在运行时，数据在计算机内存中临时存储。要把数据输入和输出，是把内存和外部存储之间通信，本质上是数据格式转化的过程。浮点数在转化时，会有损失，在考虑数据格式的时候，尤其要注意。在精度可行的前提下，考虑转化是否方便。数据转化是工具的功能，它们承载了数据中的信息，设计目标是让人方便地写入和读出。在实验进行中，我们会很乐意看看，数据文件中都记录了什么，实验仪器的状态是什么样子的。在处理过程中，我们也有判断中间结果是否合理的需求。
  因此数据格式方便与否，直接关系到研究中的日常体验。

** CSV
   CSV 是 comma separated values，本身就是文本文件，可以直接用编辑器打开。CSV 符合我们的书写习惯，每一行从左往右写，写完一行写下一行，在文本文件中把数据排成二维的表格。这种方法对整数和字符串尤其有效。现在浮点数要格外注意，用文本描述小数时，难免有精度损失，应当谨慎评估。
   二维表格是 CSV 的特点，也是种限制：它无法直接表达高维的数据。但是这个限制不本质，在本书第四章“关系代数”部分，我们会看到一切数据都可以归结为表格。
   NumPy 中自带了处理 CSV 文件的功能。这里 “comma separated values” 已经未必是“comma”（逗号）分隔了，而是广义的“分隔符”，可以是空格或制表符（TAB）甚至是句号——只要不引入歧义都可以。
   我们生成一个 (10, 10) 形状的二维数组试一试
   #+NAME: dad6dfb1-6190-48a9-8e3b-50d1d56fe216
   #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
     import numpy as np
     s100 = np.arange(100).reshape((10, 10))
     print(s100)
   #+end_src

   #+RESULTS: dad6dfb1-6190-48a9-8e3b-50d1d56fe216
   #+begin_example
   [[ 0  1  2  3  4  5  6  7  8  9]
    [10 11 12 13 14 15 16 17 18 19]
    [20 21 22 23 24 25 26 27 28 29]
    [30 31 32 33 34 35 36 37 38 39]
    [40 41 42 43 44 45 46 47 48 49]
    [50 51 52 53 54 55 56 57 58 59]
    [60 61 62 63 64 65 66 67 68 69]
    [70 71 72 73 74 75 76 77 78 79]
    [80 81 82 83 84 85 86 87 88 89]
    [90 91 92 93 94 95 96 97 98 99]]
   #+end_example

   把它存成 CSV，
   #+NAME: 1db8db20-7973-44e8-a0b4-7c47cffa1048
   #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
     np.savetxt("s100.txt", s100, fmt="%d")
   #+end_src

   #+RESULTS: 1db8db20-7973-44e8-a0b4-7c47cffa1048

   看一下它的内容
   #+NAME: b3435226-9037-437c-ab12-35c92a961a0d
   #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
     cat s100.txt
   #+end_src

   #+RESULTS: b3435226-9037-437c-ab12-35c92a961a0d
   #+begin_example
   0 1 2 3 4 5 6 7 8 9
   10 11 12 13 14 15 16 17 18 19
   20 21 22 23 24 25 26 27 28 29
   30 31 32 33 34 35 36 37 38 39
   40 41 42 43 44 45 46 47 48 49
   50 51 52 53 54 55 56 57 58 59
   60 61 62 63 64 65 66 67 68 69
   70 71 72 73 74 75 76 77 78 79
   80 81 82 83 84 85 86 87 88 89
   90 91 92 93 94 95 96 97 98 99
   #+end_example
   =savetxt= 默认的格式是 =%.18e= ，即18位的科学计数法。对整数来讲，使用 =%d= 可增加可读性。这个数字转字符串语法约定承袭于 C，叫做“printf format string”。
   输出的 CSV 文件由空格分割。每行 10 个数字，一共 10 行。直觉上看，这就是个 \(10 \times \) 的表格。一个可能的麻烦是，带空格的字符串出现在 CSV 输出里，怎么办？它的解决方案没有一定之规，有加引号的，有把分隔符换成其它字符的。遗憾的是并没有形成完整的约定，各方案未必兼容，从而解读出错误的数据。
*** 读入
    #+NAME: b513262d-b262-4b2f-b68b-f1405ec89380
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      np.loadtxt("s100.txt", dtype=int)
    #+end_src

    #+RESULTS: b513262d-b262-4b2f-b68b-f1405ec89380
    #+begin_example
    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
           [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
           [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
           [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
           [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
           [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
           [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
           [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
           [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])
    #+end_example
    =dtype=int= 保证以整数解读文件，否则数字读入时会被暗中转化成浮点型。
    
    NumPy 还给出了更多种类型，用于平衡精度与空间占用，包括整形的 =int16= =int32= =int64= ，浮点型的 ==float16= =float32= =float64= =float128= 等。数字代表了所占的比特位数。
    #+NAME: f0d3e3e0-e52e-4bf9-b4d6-adfa69ebc465
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      np.loadtxt("s100.txt", dtype=np.int16)
    #+end_src

    #+RESULTS: f0d3e3e0-e52e-4bf9-b4d6-adfa69ebc465
    #+begin_example
    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
           [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
           [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
           [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
           [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
           [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
           [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
           [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
           [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]], dtype=int16)
    #+end_example
    NumPy 数组与列表的区别，在于它一定有数据类型，即使在没有类型时，也要有统一的 =object= 类型。

    有如此多的数据类型，如何知道每个 CSV 文件适合用哪种？如果用错了，轻则会损失精度，重则会产生量级的差别。这样每次读数据之后，都要检查一遍会不会读错，检查分隔符是否用对了，“#”是否代表注释，等等。读入的正确与否没有标准，只有“与人的直觉一致”，或者在团队中有明确的约定。这是 CSV 直接用文本文体存储所带来的劣势。这一劣势伴随的优势是透明，只要能处理文本文件就可以处理 CSV。

** HDF5
   HDF5 意思是 Hierarchical Data Format 第 5 代。HDF 最初的设计目标是提供科学数据的“图形格式”标准，方便对数据研究绘图，揭示规律。科学数据的特点是规则、体量大，要求 HDF 数据格式具有高性能，并通过透明压缩减小资源占用。
   HDF 由非盈利组织开发维护。从第4代开始在各学科尤其是天体物理领域流行起来。由 NASA 选定，很多海量望远镜数据都通过 HDF 格式存储。1998 年 HDF 到了第 5 代，很多物理实验和超算中心开始采用 HDF5 。从 1.8 版本的 HDF5 开始，netCDF4 （另一个在天文观测中广泛使用的格式） 与 HDF5 正式统一。
   相比于 CSV， HDF5 的好处是带有数据类型，这样做的代价是不能按照文本文件读写了，需要专门的查看器来贯彻“透明”原则。通过制定开放的工业标准，让 HDF5 的格式良好定义，允许几乎所有程序语言的第三方程序对它进行读写，可以增强它的“透明”性。这使得从早期开始，HDF 基础之上就有大量数据分析工具涌现。
   HDF5 具有数据的原始（raw）表示，即 HDF 中保存的是与内存同样标准的整数、浮点数，不会有类似 CSV 的精度损失。HDF5 的数据类型自我描述，在读入内存时不需要额外的信息源，因为 HDF5 文件中包含了数据类型和长度等辅助信息。
   HDF5 的一个潜在缺点是无法处理中文，在它的标准在制定时只考虑了英文字符。为了保证它的兼容性，尽量不使用英文字母以外的字符。
*** HDF5 的结构
    HDF5 文件结构分三种。

    数据集 Dataset 与 NumPy 多维数组很像，数据类型多种多样可自定义。组织整理数据集要类，
可以用组 Group 。组的嵌套关系用“/”表达，语法与文件夹一致，例如 =/calibration/water/waveform= calibration 和 water 是组， waveform 是数据集。元数据 metadata 作为数据集或者组的标签，例如通过 metadata 标记 =/calibration/water= 组的温度为 25。记录实验条件信息，可以使用实验记录本。但是一次原则的指导下，最好相关的信息写在同一处， 元数据的设计正是为了提供此便利。

*** HDF5 的 Python 工具
    Python 上流行的 HDF5 工具有两种，较底层极简的 h5py 和有高级功能自定义格式的 PyTables。由于 h5py 的 HDF5 原始格式与其它语言的兼容性更强，更符合标准，我们本着透明原则选用 h5py。其它工具能正常读写数据，远比 20% 的性能提升重要。
    h5py 缩写的含义是 HDF5 Python。它的本质是 Python 调用 HDF5 C 语言库的接口，因此与使用 HDF5 标准格式无差别。

    装载 h5py ，看一下它的 test。
    #+NAME: 8427dd6c-684b-422c-a9fe-554d3420a7d1
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      import h5py
      with h5py.File("s100.h5", "w") as opt:
          opt["s100"] = s100
    #+end_src

    #+RESULTS: 8427dd6c-684b-422c-a9fe-554d3420a7d1
    在当前位置写入一个 =s100.h5= 的文件。从文件系统可访问到它。
    #+NAME: 4194fe9c-a5be-4f37-ad15-2052fe911a35
    #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
      file s100.h5
      h5dump -A s100.h5
    #+end_src

    #+RESULTS: 4194fe9c-a5be-4f37-ad15-2052fe911a35
    : s100.h5: Hierarchical Data Format (version 5) data
    : HDF5 "s100.h5" {
    : GROUP "/" {
    :    DATASET "s100" {
    :       DATATYPE  H5T_STD_I64LE
    :       DATASPACE  SIMPLE { ( 10, 10 ) / ( 10, 10 ) }
    :    }
    : }
    : }
    =file= 识别出了它是 “Hierarchical Data Format (version 5) data”， HDF5 的工具 =h5dump= 给出它的内部信息，一个 =H5T_STD_I64LE= 数据类型，即 64 位整型的， (10, 10) 数组，与 NumPy 的原始类型一致。
    不用 =-A= 参数时， =h5dump= 能查看全部的数据
    #+NAME: 99312fc2-3083-4a3a-88b3-99604c148b47
    #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
      h5dump s100.h5
    #+end_src

    #+RESULTS: 99312fc2-3083-4a3a-88b3-99604c148b47
    #+begin_example
    HDF5 "s100.h5" {
    GROUP "/" {
       DATASET "s100" {
          DATATYPE  H5T_STD_I64LE
          DATASPACE  SIMPLE { ( 10, 10 ) / ( 10, 10 ) }
          DATA {
          (0,0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
          (1,0): 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
          (2,0): 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
          (3,0): 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
          (4,0): 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,
          (5,0): 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,
          (6,0): 60, 61, 62, 63, 64, 65, 66, 67, 68, 69,
          (7,0): 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,
          (8,0): 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
          (9,0): 90, 91, 92, 93, 94, 95, 96, 97, 98, 99
          }
       }
    }
    }
    #+end_example

    =H5T_STD_I64LE= 存储 100 以内的数据太浪费，只要8位就够了。我们把 NumPy 的数组转成 8 位整型后，保存到 HDF5。
    #+NAME: 7b4ec890-c4b9-4498-ab1c-e1198e0b664b
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      with h5py.File("s100-int8.h5", "w") as opt:
          opt["s100"] = s100.astype(np.int8)
    #+end_src

    #+NAME: 32a60939-b376-4294-a3b5-55e1877decb2
    #+RESULTS: 7b4ec890-c4b9-4498-ab1c-e1198e0b664b
    #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
      h5dump s100-int8.h5
    #+end_src

    #+RESULTS: 32a60939-b376-4294-a3b5-55e1877decb2
    #+begin_example
    HDF5 "s100-int8.h5" {
    GROUP "/" {
       DATASET "s100" {
          DATATYPE  H5T_STD_I8LE
          DATASPACE  SIMPLE { ( 10, 10 ) / ( 10, 10 ) }
          DATA {
          (0,0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
          (1,0): 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
          (2,0): 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
          (3,0): 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
          (4,0): 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,
          (5,0): 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,
          (6,0): 60, 61, 62, 63, 64, 65, 66, 67, 68, 69,
          (7,0): 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,
          (8,0): 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
          (9,0): 90, 91, 92, 93, 94, 95, 96, 97, 98, 99
          }
       }
    }
    }
    #+end_example
    确认数据类型变成了 =H5T_STD_I8LE= ，但是内容不变。文件大小
    #+NAME: e9426b64-40ec-42ab-a903-695a6b495dae
    #+begin_src ein-bash :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture.ipynb :exports both
      ls -lh s100*.h5
    #+end_src

    #+RESULTS: e9426b64-40ec-42ab-a903-695a6b495dae
    : -rw-r--r-- 1 xubd xubd 2.8K Jul 19 11:51 s100.h5
    : -rw-r--r-- 1 xubd xubd 2.1K Jul 19 12:07 s100-int8.h5
    =int8= 存储的确实节省了空间，但是注意它能表示的范围只有 -128 至 127。
    
    注意，在 =h5py.File= 是大写的 =File= ，在写入数据集时，例子中使用了 =opt["s100"] = s100= ，当作字典来使用。写入的风格与 CSV 有所差异，它们相同的地方仅仅是都打开一个文件，但具体如何打开，打开后如何操作，不同的作者有不同的约定。多种形式难以记住，可随时查阅在线帮助。
*** 读取 HDF5
    #+NAME: 97fb07d8-eb6d-41d9-b595-b2547471e631
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      with h5py.File("s100.h5") as ipt:
          h5_s100 = ipt["s100"][...]
      print(h5_s100)
      print(h5_s100.dtype)
    #+end_src

    #+RESULTS: 97fb07d8-eb6d-41d9-b595-b2547471e631
    #+begin_example
    [[ 0  1  2  3  4  5  6  7  8  9]
     [10 11 12 13 14 15 16 17 18 19]
     [20 21 22 23 24 25 26 27 28 29]
     [30 31 32 33 34 35 36 37 38 39]
     [40 41 42 43 44 45 46 47 48 49]
     [50 51 52 53 54 55 56 57 58 59]
     [60 61 62 63 64 65 66 67 68 69]
     [70 71 72 73 74 75 76 77 78 79]
     [80 81 82 83 84 85 86 87 88 89]
     [90 91 92 93 94 95 96 97 98 99]]
    int64
    #+end_example

    我们也用了 =with h5py.File= ，默认是读模式。和写时一致，文件读的操作器 handler 也能当成字典使用。在调用 =ipt["s100"]= 时，后面要加 =[...]= ，代表把所有数据读进内存。
    操作器的类型是 =h5py._hl.files.File=
    #+NAME: e2610440-8ee1-4c21-9420-1a1eb4a35deb
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      type(ipt)
    #+end_src

    #+RESULTS: e2610440-8ee1-4c21-9420-1a1eb4a35deb
    : h5py._hl.files.File
    它并不是字典，但是模拟了字典的接口。这是工具接口的常见设计思想，模仿一个大家都熟悉的工具的接口。

    当我们读 int8 的文件时，HDF5 自我描述可自动把 NumPy 的类型设置好。
    #+NAME: 1f6a8fe9-ad8d-4a91-9d76-b7fcdd405a02
    #+begin_src ein-python :results output :session https://dpcg.g.airelinux.org/user/xubd/lecture-python.ipynb :exports both
      with h5py.File("s100-int8.h5") as ipt:
          h5_s100 = ipt["s100"][...]
      print(h5_s100)
      print(h5_s100.dtype)
    #+end_src

    #+RESULTS: 1f6a8fe9-ad8d-4a91-9d76-b7fcdd405a02
    #+begin_example
    [[ 0  1  2  3  4  5  6  7  8  9]
     [10 11 12 13 14 15 16 17 18 19]
     [20 21 22 23 24 25 26 27 28 29]
     [30 31 32 33 34 35 36 37 38 39]
     [40 41 42 43 44 45 46 47 48 49]
     [50 51 52 53 54 55 56 57 58 59]
     [60 61 62 63 64 65 66 67 68 69]
     [70 71 72 73 74 75 76 77 78 79]
     [80 81 82 83 84 85 86 87 88 89]
     [90 91 92 93 94 95 96 97 98 99]]
    int8
    #+end_example

我们可以 keys
我不知道
试一下
好
可以 keys
我们看这里边
类似于字典的
h5py的文件里面
有一个叫hz的元素
我们就可以把这个元素取出来
把它取出来
我看这是
它告诉我们
这是HDF5的一个dataset
名叫hz
它的是10×10的一个矩阵
它的态度
i表示整数
1表示一个字节
那就是一个字节是
是一个字节
一个字节
就是8位
正好就是int8
所以这也是python里面常见的
因为python非常
核心的数据类型
就是字典
另一个核心数据类型是列表
所以很多时候
如果一个操作的对象
它可以抽象成一个字典的话
它就会模拟成一个字典
给我们来用
如果它抽象成一个列表
就会模拟成一个列表给我们用
这样我们在使用python的时候
很多对于字典的操作
或者是函数
各种各样的工具啊
都可以无缝的移植到
比如说h5py的文件上
这是一个非常好的问题
提醒我跟大家介绍一下
它的字典的接口
我们要特别的加一个...
才能把或者是中小括号
才可以把整个的数据
读入内存内存
但是我们这门课叫做大数据方法
那么数据有些时候就会非常大
一直大到连内存都装不下
整个内存都装不下
在这种情况下
HDF5依旧是有方法
它可以把文件分块读入
虽然这个文件整个很大
没办法一次都读到内存里面
但是可以先读一块 再读这一块 
从而完成这个数据处理
不会把内存
爆掉
那么这种操作
它的学名叫做out of core computing
如果想深入了解的话
可以搜索关键字
或者 out of core learning 
就是
超大范围的
或者是大数据驱动的这种
机器学习
有兴趣的同学
可以去了解一下这个概念
我们看同样的
刚才说有两个基本的类型
很多的库都会提供这样的接口
一个是字典
一个是列表
那么numpy
也是整个python科学计算的
整个社区或者生态系统里面的
最基本的
也是最标准的数据格式
所以很多的科学计算的库
它都用numpy的语法格式
来给大家提供接口
比如说
像我们刚才打开的文件
就ipt我们看到它的类型
是HDF5的dataset
h5py里边的
一种dataset的类型
但是这种dataset的类型
它其实也有numpy的
非常多的典型的功能
比如说我看它的shape
和numpy的shape是一样的
比如说我可以看到的dtype
这是numpy数组的特点
我们看dtype也是int8
我们可以做别的
比如说取第0行
取第0行的第一列
比如说可以这样
每两行取一行
然后每三列取一列
这样都可以把它取出来
和昨天我们做的索引
都是一样的
比如说我们
好
这个不可以
它的功能
没有实现完全
应该是不可以
不能倒着来
这么着来
是因为它
特意地模拟了numpy的接口
对于大部分功能都实现了
个别的功能它没有实现
比如说倒着
倒着输出
我看比如说从第三行开始
这都可以
所以说一个
 h5py的这样一个
dataset
它和numpy的
array的用法是一样的
但是
它其实没有完全的读到内存里边
它即使对应的dataset比内存还大
我们依旧可以做这样的操作
比如说我们几个希望
取得每100行取一行
把数组取出来
那么可能我需要的内存
实际上只需要1%
但是一般的方法
我需要把它全部读到内存里面
再取得1%
现在这个文件如果在硬盘上
我就可以
让这个文件
在硬盘的前提下
把需要的东西1%取出来
这样会可以保证
内存不会坏
就说这是 out of core computing
 一个核心的数据结构
这个数据格式 hdf5
为了展示
数据对象
实际上是存在硬盘上的
这个事实
我们可以先把句柄给关了
我们把这个文件
先把它关了
关了之后
这样不好
你先把它打开
先把它打开
然后我们给它赋个值
比如说它是 
然后我们把这个文件关了之后
再访问 core_hz
我们发现它是close HDF5 dataset
比如说它已经
就文件被关闭了
所以说这个数据已经反映不到了
这时候我们要再取
其中的元素的话
它就会出问题
因为这个文件已经关闭了
所以说可见
core_hz 它是存在于硬盘上的
而不是在内存里边的
但是如果我们重新来一遍
如果我们core_hz
当我们创建core_hz的时候
把它都读到内存里面
用中括号
小括号都读到内存里面
那么core_hz我们可以看到
它其实是一个
读成了一个numpyarray
在内存里面
所以说这是HDF5的一些
针对大数据非常优化
非常实用的特性
那么刚才讲到有DATASET
不仅有DATASET
还有GROUP 我们怎么创建GROUP
用 creat_group 命令
比如说我们说
要输入
输出这个文件 hzg
那么我们创建一下
create_grou[
这个group叫做 home
这样就有了一个home
有了home之后
就相当于
每个group相当于一个字典
相当于我找到home之后
这有一个新的字典
我把它刚才的文件
刚才数组把它放进去
应该没问题
我看一下
我们看一下
是我们刚刚创建的文件
这个文件和刚才的文件没什么
区别
区别就在于我在这里边
加了一个叫做home的group
而dataset
放到了 group里面
这是唯一的区别
那么从 group里面
我们再把它读出来
是一样的
比如说
with
然后把它读出来
再把 dataset 读出来
再把它都放到内存里
这样我们就把它读出来
它为什么变成了int64呢
没有问题
好
关于HDF5大家有什么问题吗
包括刚才的写入和读出
还有组
我们暂时我们用不到metadata
等一下我们遇到的时候
再去详细的讲
然后看来大家都没有疑问
因为最开始我们没有创建组
它就有一个默认的
全局的组在那
那个组我们其实看h5dump
我们看到有一个全局的组
像 POSIX 文件系统的
根目录一样
就是斜杠表示全局的组
这是我们带home组的
如果不带home组
我们看
它还是有一个默认的组在这
调整分组
可以
刚才同学的问题是说
如果我对于这样的一个
DATASET hz  
它在home组里边
我如果把它想移动到其他的组里
比如说
这个根组怎么办呢
移动的时候
其实复制一步
再把原来的删除就可以了
没有一个特殊的移动的程序
但是在操作的内部
它没有把这个数据拷贝过来
它只是改了一下
链接的指针
我们可以试一下
比如说
我们要做一个hz的移动
把它从home组移到这个根图里面
我们把它打开
这样它就不是
比如说我以
我以读写的方式打开
不可以读写
那么我看一下
它里面有home
我开错了
应该开那个g
然后我看这里边有home
Home里面有一个member
这叫hz
有同学探索了一个新的方法
会这样
这是一样的
而且之前我们省略了最顶层的
如果在这加个斜杠
也应该是一样的
所以可见它有非常多的写法
在Python里面
它可以抽象成一个字典
套另一个字典
HDF5它的原始的
这种语法里边
它可以用一个像路径的格式来写
比如说
我们可以把它移动到根的组里
就这样被移动了
这个时候应该
不涉及数据的拷贝
然后我们再把它删掉
应该是这样del
就可以把它删掉了
然后我们把文件close
我做了三个操作
第一个操作是说
把home里边的hz放到这个全局
group里边
第二步操作是把原来的删掉
第三步是把它关掉
我们看一下
现在根的group
它里边有两个元素
一个元素是一个空的
叫做home的group
另一个元素是我刚才移过来的
DATASET
这样就可以完成移动的操作
它没有一个
我还不知道
不知道它有没有移动的操作
专门的移动操作
但是可以先复制
再删除
完成
复制的时候
大家也不用担心
它不是说
把整个的文件
又生成了一遍
复制过去
应该只是更新的一个指针
大家还有什么疑问
关于HDF5 我们可以看到HDF5
它的表现力
其实就非常强的
它甚至里边有一个
像目录结构的东西
那么HDF5
几乎可以表达出我们所遇到的
所有的实验数据
目前我还没有看到过反例
而它又是一个开放的标准
那么所以说
HDF5就是大规模数据处理
非常
非常方便的格式
值得一提的是
 Matlab
有一种data的数据格式
叫做.mat文件
这个.mat文件
其实就是HDF5 所以说可见
hdf5格式
在整个的
不论是工业界
还是学术界
影响
都是非常深远
而且因为HDF5 它可以由其他的语言读进来
所以说
用python处理的一些 无论是输入
还是输出
那么有了HDF5之后
即使你的
你的队友不会用python
他会用Matlab
那么你可以
你们之间可以
通过一个数据的流水线
用HDF5把它连接起来
其中你们交换HDF5
或者说它不会Matlab
只会R
那么用HDF5也是可以的
最后一个格式
我会讲一下json
json的作者
其中曾经给叫做json的人道过歉
说给他们的生活
造成了很多的不便
继续
比如说你在生活中
经常听到别人喊自己的名字
然后也是一种
所以作者之前给他道过歉
说
他当时没想到
他创造的标准
能够流行的这么广泛
那么json是什么
它是 JavaScript
Object Notation 的缩写
然后可以看到
它其实是和
 JavaScript 有
非常深的渊源
那么JavaScript
相信
做一点网站
或者网页开发的同学
会有印象有
JavaScript
就是做网站前端的
目前来说的
最受欢迎的语言
那么即使不做网站开发
我们每天每时每刻都在用
JavaScript
只要我们上网的话
再开浏览器的话
基本上都会用到JavaScript
比如说现在我播放的 PPT
PPT
不是一个严格的词
我播放的讲稿
它就是用JavaScript
实现的
那么最开始JSON数据结构
它是
从做网站的需求来的
比如说
我要在网页里面
显示一些动态更新的内容
那么这些动态更新的内容
肯定是用户输入的
或者是从哪里抓取
那么它这些内容
肯定是有一个来源
比如说从某个数据库
提取出来
或者是从别的地方传输过来
但传输肯定有一个传输的格式
这个格式在JSON出现之前
是用xml传输的 
xml的这种格式
它其实设计的也挺不错的
但是它有个弊端
它不具有透明性
也就是说
当这个格式
变得很复杂的时候
人类一读他就会非常的头大
所以说开发者就非常不喜欢
 xml
当JSON出现之后
人类也可以读懂
机器
也可以读懂
所以人类和机器
就更加和谐的相处
可以更加相互理解了
所以说很快 xml就被
网站开发的社区抛弃
现在虽然还残留些xml
基本上新的工具都是基于JSON
那么JSON
后来也成为了一个国际的标准
这使得网站开发中的数据交换
更益于人类
理解
而且它非常适合传递
有层次的数据
特别是像文本这种类型
特别是像网站里边
这些文本
从数据库提取出来的文本
然后可能是有段落
我第一级第二级第三级
这种结构非常适合用JSON来表达
那么JSON优点
它是跟 Python的字典
非常相近
一会我们会看到
 JSON的例子也是一样的
它在python里边
它就是伪装成了一个字典
我们根本很难把它区分开
我们就把它当成字典
用就行
那么它的缺点
因为它依旧是一个纯文本
它是纯文本
那么它就需要把数字
特别是浮点数转化成文本
转化的过程会有误差
所以它对数字的表达能力
其实是比较弱的
因此在这种非科学的情况
在这个网站里面
JSON比较多
非数值的情况
那么科学的情况
用JSON其实也有不少
因为在一个大的科学实验里面
不仅有采过来的数
还有别的信息
比如说
这个实验的
比如说什么
比如说每一个事例
然后我们加一个什么代号
然后元数据 metadata
可以用JSON来传
比如说我采集下来的HDF5有
一大批目录
那么HDF5本身
我们可以用HDF5的metadata
也可以用JSON来做一个索引
这些我们一会儿
接下来应该会碰到这些例子
到时候我们再具体去讲
现在讲可能会比较
空中楼阁
总之JSON的数据格式
有这样的特点
好
怎么使用JSON JSON目前是python
自带的
我请同学们下载一个文件
叫做
BBH_events_v3.json
网络学堂
大家如果手头有JSON的话
就不用下载这个文件
如果你手头没有JSON文件的话
就下载一个样例文件
LIGO数据集下载
这里有下载地址
大家先不要下载那些大文件
因为我们今天的网络稍微差一点
这个JSON文件就是这个样子
这里有
这是清华网盘可以看到的
文件
我们看一下
这个文件
首先JSON的格式
它其实就是一个纯文本
那么纯文本有一点结构
这里有个括号
括号里面
第一个
然后有个冒号
后边还有个括号
看起来是不是非常像
 Python的字典
这就是它的
它的键
这就是它的值
这是它的键
这是它的值
而且键值对之间
也是用逗号
分割的
它简直就是python的字典
然后看
字典的里边
这个键对应的值
它又是一个字典
这个字典还是可以跟字典进行
看起来是可以进行嵌套的
然后这里边有各种的数值
它的name
这里都是字符串
这里还可以是简单的数字
这里还可以嵌入列表
啊这个列表我们看
竟然和Python的列表
也完全一样的语法
我怀疑这个作者
当时是受了python的影响
我不知道
当然大家可以考察一下这段历史
我们看这个东西
直接拿过来就可以用了
就可以读到python里
我们来把它
大家先下载一下文件
 Download
2.2KB 如果大家手头
有其他的JSON文件
也都可以
不是非得读这个文件
好
那么我们把这个文件下载
下载下来
放在我们现在工作的目录下面
大家知道
你工作的是哪个目录吗
好像不知道
放在一个你知道的地方
是吧
比如说我用的目录在这里
我已经下载了
给大家
给大家两分钟时间
你把这个文件
放到一个你知道的地方
用 windows的同学把
这个文件
下载到一个文件夹之后
你可以找到
它在windows里面的路径
然后你可以在
 WSL里边
访问目录mnt
然后比如说是
c 或者 d 或者 e
然后你再访问什么
比如说Document
另外一种可能
你可以用VScode的那个remote
在你的工作目录里面
新建一个文件
新建文件之后
把JSON复制进去
也可以
但是你要找到
你把这个文件放那了
好
我应该是能找到的
我就把它放在了
我的当前路径里面
import json
就把JSON读进来了
 json.load
我刚才是把我的
比如说我用一个完整的路径
它是在这里
当然你要把它换成
你所知道的json文件的位置
 大家不要看我路径
每个人的路径都不一样
你只要找到这个文件就行
如果这个文件就在
启动python的当前目录里边
那么这些就都不用打了
就这样打就可以了
比如说我把它取名叫
如果你把它保存到
比如说C盘还是D盘
你就要看一下你
这个文件的属性里面
有一个路径名
然后
你再从 WSL的路径数过去
找到那个文件
把文件的路径名写到
这里
我刚才在下面发现
同学们好像对这个操作不是很
熟练
所以说
稍微有点担心
同学们都能把它读进来了吗
不能把它读进来
也是这样的
你要用这种斜杠
要用从右上到左下的斜杠
你要这样打进来
但是你要找
路径在哪
你可以ls
比如说一点一点把它
把它找出来
摁TAB 一点点把它找出来
比如说我的是在这里
然后这里
这里
这里就是这个文件
然后我比如说可以FILE看一下
然后JSON
我把这个文件拿过来
放到loadopen
文件名里面
发生了什么
文件名过长了
我们发现
还会出现这种情况
这个配色太糟了
配色太糟糕
我可以先到路径里
比如说刚才我找到的路径
先到这个路径里
然后再执行python
然后再read出来
这回就好了
我有打错
应该是load
不是read
这样一个命令
然后同学们这个路径
你一定要用你的路径
不要用我的路径
同学们都能读入吗
不能读进来的同学请举手
都遇到什么问题
有一个同学的问题是
他直接把路径的字符串
放到这个load里了
所以犯了跟我刚才一样的错误
你需要先把文件open出来
然后才能load
大家要注意
我这里面有两个函数
一个是json.load
load传的参数
是一个open的函数的返回值
这是两个函数嵌套出来的
还有一个同学遇到的问题
是mnt在找路径的时候
因为windows里边
你的资源管理器
或者我的电脑打开之后
 C D E都是大写的
但是在WSL里面
这些都是小写的
来注意这个盘符是小写
哪位同学没读进来
刚才遇到了一个问题
一位同学找到了WSL对应的
 windows的路径
路径可能是什么
然后 wsl什么途径
然后把 json 文件存到路径里了
然后希望能够从 wsl 的环境里
出现在 home  wsl的home里
但是事实证明
这个方法是行不通的
因为WSL它的原因是
 windows的文件系统
它不支持POSIX
标准环境接口
所以说
在WSL里边
模拟一个POSIX的时候
它并不能直接使用
 Windows里边的硬盘
它是加了很多抽象
之后才能用
所以你目前
WSL还没有这个功能
你直接把文件放在它的底下
它的上面应该是看不到这个东西
这个是我猜的原因
大概是这样
所以说请使用这种方法
或者是把文件粘到这个环境里
那大家都可以读这个文件了吗
刚才有些同学遇到的问题是
mnt前面一定要有斜杠
不能忽略
还有哪位同学
无法读出找到json文件
基本上都解决了
刚才遇到了几个共性的问题
给大家解释一下
那么把文件从windows传
到 WSL的操作
还是很重要的
我们会经常的
把数据文件放在这里
所以稍微比较可靠的
然后推荐用这个方法
因为刚才其他的方法
比如说在数据里边
不是在WSL里面
粘贴进去
对于这种json的小文件可以
但是如果特别大的文件
还是这样来做比较可靠一些
或者wget
也行
但是清华网盘能wget
我们下一批数据文件
我们wget
这个问题还是比我想象的复杂
刚才遇到困难的同学
现在都解决了吗
有没有还没解决的
好
大家都可以找到json了
是吧
我们每个人自己的路径
我们把它读起来
叫做evts 我们看一下evts是
什么样的
看起来很乱
那么我们一点一点看
比如说 evts.keys
看他里边都有什么keys
这里边有这个
GW150914
然后VT151012
是吧
那么我们来对比一下
它其实就是这几个keys
就是这里边的 json的这些key
这些键
就是这样的
然后因为它就是一个字典
所以我们就可以把第一组 
GW150014把它读出来
看能不能读得好一点
没办法
它就是一个字典
我们回到我们的
这个里边
这个读出了keys
然后我们看一下keys里边都
有什么
有name 有 H1 L1
 fs 这样
我们对比一下这个文件
其实都是一样的是吧
没问题
那么这个json
其实就是一个当做字典来用的
数据形式
那么接下来
我们还可以把它输出出去
比如说dump就是输出
我们可以看到
numpy用的是loadtxt
savetxt
HDF5 就是一个大写的File
结果json它是有
dump 和 load
大家用的词都不一样
所以说很容易来记混
所以大家使用的时候注意一下
我看dump是要怎么用
我忘了他怎么用了
看一下
dump 是 object
这里面放变量
这个地方放文件
然后就可以到
我们看
前面object 我们的那个是
evts
然后是文件
我们先要把它打开
以写的模式打开
那么这个文件我们新命名
比如说叫
然后把它写进去
看起来我这个地方还不能写b
不知道windows是什么
情况
Windows是需要打b的吗
同学们都成功了吗
没打b就成功了是吧
看来只有numpy是要打b
这个太玄学了
我们来看一下这个
刚才新出来的文件是 BBH_rewrite.json
我们看一下
你看rewrite成了一个这样紧凑的
格式
但是对于人类来说
看看的不太明白
是吧
虽然它们是一样的
我们再看一下
这个dump函数
有没有什么其他的选项
能够让它漂亮一点
比如说 
看起来缩进
这个看起来很像
是吧
我们需要的漂亮的输出
在indent level的时候
你看indent它
这里indent的参数
它默认是None
默认
这个情况下是 the most compact
representation
所以我们来换一下
让它indent
让缩进两个格
缩进这两个格
它看起来就可以漂亮一些
和之前的一样
比如说我们看它就是把嵌套的
字典
能够把它输出到硬盘上
这是
这是 json dump 做到的
同学们都成功了吗
遇到困难的请举手
都没有遇到困难是吗
这是json load的时候
把这个文件名
load进来
就可以得到这些
然后dump
把evts可以dump到文件里面去
这是
两个
命令
所以我们看到了三种数据格式
那么三种数据格式
最简单的是CSV
CSV一般来说
我们如果没有特别需求
其实可以用CSV 还是挺好
因为至少CSV还可以用
 Excel打开
可以很直观的
用这个表格功能来
来处理它
那么CSV他
它有很多问题
它第一个问题是
它只能表达表格
如果我们
不是表格的话
如果它不是表格
如果它不是表格的话
一般我们会用json 因为毕竟字典
把字典进行嵌套
可以表达出来非常多的数据结构
那么如果我们要求这个数值输入
一般我们会用HDF5
一般来说是这样取舍
对于我们实验物理这个领域
一般来说
HDF5会用的比较多
因为大多数这个实验数据
都是数据型
对于一些小规模的数据
或者是规模大
大数据
变大的话
就变成一个HDF6
而小规模的数据
或者是比较简单的数据
如果能用CSV我们就可以用
CSV
那么他们的优缺点
json 它主要缺点
也是数值的问题
所以说
如果它不是
然后还是需要
数值计算的话
那么也可以从json换到hdf5
所以说终极的 可以这样认为
终极的解决方案可以是HDF5
虽然它是终级的解决方案
它就有一个很大的缺点
它不是透明的
我们直接用nano什么的打开
看不到
它里面到底是什么数据
我们还得用一些特殊的工具
比如说h5dump
比如说python
把它读出来
但是对于这些特殊的工具
已经非常普遍了
非常普及了
在各个平台都有可以非常自由的
取得
而且有多种多样的工具
所以说在这种情况下
它的透明规则的破坏了
也被伤害也降到了最低线
所以说
从复杂性算HDF5是最复杂的
然后他也是功能最强大的
所以说
在实际的情况下我们看
什么时候适合用什么样的格式
这个大概
希望同学们能够啊
有一个初步的判断和选择的
基本的规则
所以接下来
我们会做一个
关于HDF5的作业
这个作业是在
网络学堂
我们来实际操作一下 HDF5
它的输入输出
对吧
我们看一下作业
这个作业是干什么的
问题背景
问题背景
我们可以跳过了
问题描述就是说
我们要输入一个文件
然后PPhappy 下面有个PPMatrix
这有个数据集
这个数据集我们读到
读到python里边
然后用numpy给矩阵进行
转置
转置了之后
再把这个文件写入
另一个文件
那么写完了之后
就可以完成了
所以说
这个作业也是非常的
逻辑上非常简单
所以考察大家
只是对于HDF5的操作
我还是不带着大家做了
感觉这个作业还是非常的简单的
我看一下
把 HDF5文件读入
然后对它进行一个操作
再输出就可以了
在我们课上已经讲到了这些
些个操作
然后这里边
可能遇到的
比如说create_group
还有create_dataset
可以用助教
给的默认的dataset
就没问题
然后到底怎么做矩阵转置
大家可以搜索一下
如何进行
数据的格式的基础
其实我们就已经把大作业需要的
基本的
基础的技能
都已经准备好了
所以
在明天会把大作业的具体要求
发布出来
包括输入数据的格式
应该大多数都是HDF5的格式
输出也是会用HDF5来输出
那么前两周的课程
对应的大作业的第一个阶段
在物理场景下
把这些数据
从真实的世界中的值
一步一步生成出来
生成出模拟的经过实验仪器
所收集下来的这种数据形式
也就是说生成过程
然后第三四周的课程进入
大作业的第二个阶段
用于把这个过程倒过来
进行数据的分析
就是从我们实验仪器
所收集到的信息
反推最开始生成它的数据
那么分析数据这个过程
都是开放的问题
肯定大家有无限的探索空间
甚至肯定
我相信肯定会有同学的方法
会比我的更好
我不清楚
我们明天来具体把这个定义一下
定义出来
剩下的时间
同学们可以做一下作业
然后做作业的时候
遇到什么问题
可以跟大家讨论
或者是跟我提问一下
没有问题的同学可以先下课
** JSON
   当数据没有整齐形态，可能伴随有分支、嵌套等时，使用json更方便。
* 软件管理器
  我们的 GNU 系统环境中，都带有软件管理器，例如 =apt= 。需要什么工具可以随手安装，只要网络足够快，就能快速安装和使用，非常方便。这个工具叫包管理器 package manager 。
  
